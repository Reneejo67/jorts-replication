{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a07bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jorts_utils import *\n",
    "import json\n",
    "\n",
    "# Loading JKR's partial follower list\n",
    "DATA_PREFIX = './data'\n",
    "\n",
    "jkr_followers_time_bounded = json.load(\n",
    "    open(\n",
    "        '{}/HASHED_jkr_followers_full_past_20180615.json'.format(DATA_PREFIX), 'rb'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Map each follower user ID to the time at which they followed JKR. \n",
    "jkr_followers_ts = {}\n",
    "for ts, followers_gained in jkr_followers_time_bounded.items():\n",
    "    for f in followers_gained:\n",
    "        jkr_followers_ts[f] = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97bdebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_post_2018 = pd.read_csv('{}/HASHED_jkr_rts_past_20180615.tsv'.format(DATA_PREFIX), sep='\\t')\n",
    "rts_post_2018 = df_post_2018['hashed_user_id'].tolist() # retweeted handles\n",
    "tss_post_2018 = df_post_2018['created_at'].apply(ts_hyphen_transform) # timestamps of RTs\n",
    "prefix = '{}/jkr_json/'.format(DATA_PREFIX)\n",
    "\n",
    "\"\"\"\n",
    "These data structures map each followed account to a dict with the following structure:\n",
    "{\n",
    "    True: {\n",
    "        ts1: estimated_jkr_followers_gained_on_ts1,\n",
    "        ts2: estimated_jkr_followers_gained_on_ts2,\n",
    "        ...\n",
    "        ts14: estimated_jkr_followers_gained_on_ts14,\n",
    "    },\n",
    "    False: {\n",
    "        ts1: estimated_non_jkr_followers_gained_on_ts1,\n",
    "        ts2: estimated_non_jkr_followers_gained_on_ts2,\n",
    "        ...\n",
    "        ts14: estimated_non_jkr_followers_gained_on_ts14,\n",
    "    },\n",
    "}\n",
    "For followers_gained_post_jkr, ts1 through ts14 are the days of the retweet until 14 days after it.\n",
    "For followers_gained_pre_jkr, ts1 is 14 days before the retweet up until the day before the retweet.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Here we pull full sets of followers who followed within the two-week period, \n",
    "with the aim of producing a mapping of dates the attention broker retweeted\n",
    "to the set of hashed user IDs who followed a retweeted account.\n",
    "\n",
    "With these \"samples\" in hand, we can simulate a mark/recapture experiment. \n",
    "Each day of retweets is one capturing/marking run. \n",
    "Each time we see a user's hashed ID, we \"mark\" them; \n",
    "if we encounter them again, it is a \"recapture\" event.\n",
    "\n",
    "We can then use Project MARK's free software and R package to estimate the total population\n",
    "of eligible followers. \n",
    "\"\"\"\n",
    "\n",
    "non_ab_followers_ts = {}\n",
    "ab_followers_ts = {}\n",
    "for rt_acct, rt_ts in zip(rts_post_2018, tss_post_2018):\n",
    "    amplify_ts_dt = dt.datetime.strptime(rt_ts, '%Y%m%d') # time of amplification\n",
    "    after_ts_dt = amplify_ts_dt + dt.timedelta(weeks=2) # 2 weeks after; cutoff\n",
    "    before_ts_dt = amplify_ts_dt - dt.timedelta(weeks=2) # 2 weeks prior; cutoff\n",
    "\n",
    "    after_ts = dt.datetime.strftime(after_ts_dt, '%Y%m%d')\n",
    "    before_ts = dt.datetime.strftime(before_ts_dt, '%Y%m%d')\n",
    "\n",
    "    follows_on = json.load(\n",
    "        open(prefix + 'HASHED_{}_following_data_pre_{}_all.json'.format(\n",
    "            rt_acct, rt_ts\n",
    "        ), 'r',\n",
    "    )) \n",
    "\n",
    "\n",
    "    follows_before = json.load(\n",
    "        open(prefix + 'HASHED_{}_following_data_pre_{}_all.json'.format(\n",
    "            rt_acct, before_ts\n",
    "        ), 'r',\n",
    "    ))\n",
    "    \n",
    "\n",
    "\n",
    "    follows_after = json.load(\n",
    "        open(prefix + 'HASHED_{}_following_data_pre_{}_all.json'.format(\n",
    "            rt_acct, after_ts\n",
    "        ), 'r',\n",
    "    ))\n",
    "    follows_on_set = set()\n",
    "    for v in follows_on.values():\n",
    "        follows_on_set = follows_on_set.union(set(v))\n",
    "        \n",
    "    follows_before_set = set()\n",
    "    for v in follows_before.values():\n",
    "        follows_before_set = follows_before_set.union(set(v))\n",
    "    \n",
    "    \n",
    "    follows_after_set = set()\n",
    "    for v in follows_after.values():\n",
    "        follows_after_set = follows_after_set.union(set(v))\n",
    "        \n",
    "    tot_followers = list(set().union(*(follows_on_set, follows_before_set, follows_after_set)))\n",
    "    ab_followers = [f for f in tot_followers if f in jkr_followers_ts]\n",
    "    not_ab_followers = [f for f in tot_followers if f not in jkr_followers_ts]\n",
    "    if rt_ts in non_ab_followers_ts:\n",
    "        non_ab_followers_ts[rt_ts] += not_ab_followers\n",
    "    else:\n",
    "        non_ab_followers_ts[rt_ts] = not_ab_followers\n",
    "    if rt_ts in ab_followers_ts:\n",
    "        ab_followers_ts[rt_ts]+= ab_followers\n",
    "    else:\n",
    "        ab_followers_ts[rt_ts] = ab_followers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e034cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_non_ab_followers = set()\n",
    "for v in non_ab_followers_ts.values():\n",
    "    tot_non_ab_followers = tot_non_ab_followers.union(set(v))\n",
    "    \n",
    "tot_non_ab_followers_to_seqs = {k: [] for k in list(tot_non_ab_followers)}\n",
    "\n",
    "# construct sequences of captures/non-captures for non-followers of the attention broker;\n",
    "# a 1 means the user was \"captured\" (i.e. followed an account on that day)\n",
    "# and a zero means the user was not \"captured\" on that particular day.\n",
    "\n",
    "for k, v in sorted(non_ab_followers_ts.items(), key=lambda b: b[0]):\n",
    "    in_list = set(v)\n",
    "    for t in tot_non_ab_followers_to_seqs.keys():\n",
    "        if t in in_list:\n",
    "            tot_non_ab_followers_to_seqs[t] += [1]\n",
    "        else:\n",
    "            tot_non_ab_followers_to_seqs[t] += [0]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b87172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# this is mostly a sanity check to make sure we have some recapture events.\n",
    "c = Counter([''.join([str(vv) for vv in v]) for v in tot_non_ab_followers_to_seqs.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f650dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all capture histories to disk\n",
    "with open('jkr_non_f_outf_indiv.txt', 'w') as f:\n",
    "    for seq, ct in c.items():\n",
    "        for s in range(ct):\n",
    "            f.write(seq + ';\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0871398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a 10% random sample of individuals captured\n",
    "# MARK can't handle a gigabyte of data, which is what the full dataset requires, \n",
    "# so we take a 10% random sample of capture histories. \n",
    "# we then multiply the population number MARK returns by 10 to get our final result.\n",
    "import random\n",
    "with open('./jkr_non_f_outf_indiv.txt', 'r') as f:\n",
    "    with open('ten_pct_jkr_non_outf_indiv.txt', 'w') as outf:\n",
    "        for line in f.readlines():\n",
    "            if random.randint(0, 10) == 5:\n",
    "                outf.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4373bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct capture histories for followers of the attention broker\n",
    "tot_ab_followers = set()\n",
    "for v in ab_followers_ts.values():\n",
    "    tot_ab_followers = tot_ab_followers.union(set(v))\n",
    "    \n",
    "tot_ab_followers_to_seqs = {k: [] for k in list(tot_ab_followers)}\n",
    "\n",
    "for k, v in sorted(ab_followers_ts.items(), key=lambda b: b[0]):\n",
    "    in_list = set(v)\n",
    "    for t in tot_ab_followers_to_seqs.keys():\n",
    "        if t in in_list:\n",
    "            tot_ab_followers_to_seqs[t] += [1]\n",
    "        else:\n",
    "            tot_ab_followers_to_seqs[t] += [0]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a6c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter([''.join([str(vv) for vv in v]) for v in tot_ab_followers_to_seqs.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ba454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write capture histories to disk. \n",
    "with open('jkr_foll_outf_indiv.txt', 'w') as f:\n",
    "    for seq, ct in c.items():\n",
    "        for s in range(ct):\n",
    "            f.write(seq + ';\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f28553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results, using RMark and the POPAN model.\n",
    "# followers: 841164.4\n",
    "# non-followers: 26758530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d392cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a1f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
