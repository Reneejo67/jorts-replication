{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9710c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jorts_utils import *\n",
    "DATA_PREFIX = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e230458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# loading Jorts' follower list\n",
    "jorts_followers_time_bounded = json.load(\n",
    "    open('{}/HASHED_jorts_follower_data_by_cursor_all.json'.format(DATA_PREFIX), 'r')\n",
    ")\n",
    "jorts_followers_ts = {}\n",
    "for ts, followers_gained in jorts_followers_time_bounded.items():\n",
    "    for f in followers_gained:\n",
    "        jorts_followers_ts[f] = ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "519f0183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './data/jorts/HASHED_20d9e4cae225fb9b1c0774dd5c72f6e848b89594_following_data_2_weeks_after_20220115_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_53b53a14d8d76abac1134a11853ece2a8b9efdad_following_data_2_weeks_after_20220301_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_e9efd0343979c4c8a6028bd232189bfb5e3e0309_following_data_2_weeks_after_20220224_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_2b9769fd1303dd095c1880420cfcb7ae769b1727_following_data_2_weeks_after_20220220_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_f7766c1a6617cb542922b1570f49aec461235af2_following_data_2_weeks_after_20220209_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_fed00522b7bb4fd15df5632173a0c7d1a67b654d_following_data_2_weeks_after_20220214_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_3680d5ebbb74e32b9c78b18dcd331b1cc80dcdff_following_data_2_weeks_after_20220214_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_a24389f88e7cf2dadeaf50cbf9f938229c4516ff_following_data_2_weeks_after_20220212_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_4fd65d054125e74959e7997d3f5f76518c088766_following_data_2_weeks_after_20220126_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_5c04089b61159b640095d1e798b401e37b8596f9_following_data_2_weeks_after_20220119_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_2b2b4d63c4b8dee41dec100700ce15fa959e645b_following_data_2_weeks_after_20220119_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_71af9b1be4e663a9f3eb5a4b363d33fea0742db4_following_data_2_weeks_after_20220118_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_4a0c0318a8e7aaeb261ec6d0f01b6b5fc1a8ebb7_following_data_2_weeks_after_20211216_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_e8173cb32ace92a974d0c6ae547d1253a9867213_following_data_2_weeks_after_20220102_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_dd98f5000e9e2823600ce6bf2d700f5e7861366a_following_data_2_weeks_after_20220101_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_2e6af3648829294a3969df55bd6d78eda57b9067_following_data_2_weeks_after_20211224_all.json'\n",
      "[Errno 2] No such file or directory: './data/jorts/HASHED_069e6932d0f400c224260e848e53bd6e26cd291b_following_data_2_weeks_after_20211224_all.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rt_to_ts = json.load(open('{}/HASHED_jorts_rt_authors_to_ts.json'.format(DATA_PREFIX), 'r'))\n",
    "prefix = '{}/jorts/'.format(DATA_PREFIX)\n",
    "\n",
    "\"\"\"\n",
    "These data structures map each followed account to a dict with the following structure:\n",
    "{\n",
    "    True: {\n",
    "        ts1: estimated_jorts_followers_gained_on_ts1,\n",
    "        ts2: estimated_jorts_followers_gained_on_ts2,\n",
    "        ...\n",
    "        ts14: estimated_jorts_followers_gained_on_ts14,\n",
    "    },\n",
    "    False: {\n",
    "        ts1: estimated_non_jorts_followers_gained_on_ts1,\n",
    "        ts2: estimated_non_jorts_followers_gained_on_ts2,\n",
    "        ...\n",
    "        ts14: estimated_non_jorts_followers_gained_on_ts14,\n",
    "    },\n",
    "}\n",
    "For followers_gained_post_jorts, ts1 through ts14 are the days of the retweet until 14 days after it.\n",
    "For followers_gained_pre_jorts, ts1 is 14 days before the retweet up until the day before the retweet.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Here we pull full sets of followers who followed within the two-week period, \n",
    "with the aim of producing a mapping of dates the attention broker retweeted\n",
    "to the set of hashed user IDs who followed a retweeted account.\n",
    "\n",
    "With these \"samples\" in hand, we can simulate a mark/recapture experiment. \n",
    "Each day of retweets is one capturing/marking run. \n",
    "Each time we see a user's hashed ID, we \"mark\" them; \n",
    "if we encounter them again, it is a \"recapture\" event.\n",
    "\n",
    "We can then use Project MARK's free software and R package to estimate the total population\n",
    "of eligible followers. \n",
    "\"\"\"\n",
    "\n",
    "non_ab_followers_ts = {}\n",
    "ab_followers_ts = {}\n",
    "for rt_acct, rt_ts in rt_to_ts.items(): \n",
    "    try:\n",
    "        follows_after = json.load(\n",
    "            open(prefix + 'HASHED_{}_following_data_2_weeks_after_{}_all.json'.format(\n",
    "                rt_acct, rt_ts\n",
    "            ), 'r'\n",
    "        ))\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    follows_after_set = set()\n",
    "    for v in follows_after.values():\n",
    "        follows_after_set = follows_after_set.union(set(v))\n",
    "    try:\n",
    "        follows_on = json.load(\n",
    "            open(prefix + 'HASHED_{}_following_data_post_{}_all.json'.format(\n",
    "                rt_acct, rt_ts\n",
    "            ), 'r'\n",
    "        )) \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    follows_on_set = set()\n",
    "    for v in follows_on.values():\n",
    "        follows_on_set = follows_on_set.union(set(v))\n",
    "    follows_before = json.load(\n",
    "        open(prefix + 'HASHED_{}_following_data_2_weeks_pre_{}_all.json'.format(\n",
    "            rt_acct, rt_ts\n",
    "        ), 'r'\n",
    "    ))\n",
    "    follows_before_set = set()\n",
    "    for v in follows_before.values():\n",
    "        follows_before_set = follows_before_set.union(set(v))\n",
    "    tot_followers = list(set().union(*(follows_on_set, follows_before_set, follows_after_set)))\n",
    "    ab_followers = [f for f in tot_followers if f in jorts_followers_ts]\n",
    "    not_ab_followers = [f for f in tot_followers if f not in jorts_followers_ts]\n",
    "    if rt_ts in non_ab_followers_ts:\n",
    "        non_ab_followers_ts[rt_ts] += not_ab_followers\n",
    "    else:\n",
    "        non_ab_followers_ts[rt_ts] = not_ab_followers\n",
    "    if rt_ts in ab_followers_ts:\n",
    "        ab_followers_ts[rt_ts]+= ab_followers\n",
    "    else:\n",
    "        ab_followers_ts[rt_ts] = ab_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8046bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct sequences of captures/non-captures for non-followers of the attention broker;\n",
    "# a 1 means the user was \"captured\" (i.e. followed an account on that day)\n",
    "# and a zero means the user was not \"captured\" on that particular day.\n",
    "\n",
    "tot_non_ab_followers = set()\n",
    "for v in non_ab_followers_ts.values():\n",
    "    tot_non_ab_followers = tot_non_ab_followers.union(set(v))\n",
    "    \n",
    "tot_non_ab_followers_to_seqs = {k: [] for k in list(tot_non_ab_followers)}\n",
    "\n",
    "for k, v in sorted(non_ab_followers_ts.items(), key=lambda b: b[0]):\n",
    "    in_list = set(v)\n",
    "    for t in tot_non_ab_followers_to_seqs.keys():\n",
    "        if t in in_list:\n",
    "            tot_non_ab_followers_to_seqs[t] += [1]\n",
    "        else:\n",
    "            tot_non_ab_followers_to_seqs[t] += [0]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ed16999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# this is mostly a sanity check to make sure we have some recapture events.\n",
    "c = Counter([''.join([str(vv) for vv in v]) for v in tot_non_ab_followers_to_seqs.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "707c4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all capture histories to disk\n",
    "\n",
    "with open('non_f_outf_indiv.txt', 'w') as f:\n",
    "    for seq, ct in c.items():\n",
    "        for s in range(ct):\n",
    "            f.write(seq + ';\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06464322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct capture histories for followers of the attention broker\n",
    "\n",
    "tot_ab_followers = set()\n",
    "for v in ab_followers_ts.values():\n",
    "    tot_ab_followers = tot_ab_followers.union(set(v))\n",
    "    \n",
    "tot_ab_followers_to_seqs = {k: [] for k in list(tot_ab_followers)}\n",
    "\n",
    "for k, v in sorted(ab_followers_ts.items(), key=lambda b: b[0]):\n",
    "    in_list = set(v)\n",
    "    for t in tot_ab_followers_to_seqs.keys():\n",
    "        if t in in_list:\n",
    "            tot_ab_followers_to_seqs[t] += [1]\n",
    "        else:\n",
    "            tot_ab_followers_to_seqs[t] += [0]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2e2830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "from collections import Counter\n",
    "c = Counter([''.join([str(vv) for vv in v]) for v in tot_ab_followers_to_seqs.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f574ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all capture histories to disk\n",
    "with open('ab_foll_outf_indiv.txt', 'w') as f:\n",
    "    for seq, ct in c.items():\n",
    "        for s in range(ct):\n",
    "            f.write(seq + ';\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19dfc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population estimates using RMark and a POPAN model:\n",
    "# Non-Followers: 17890822 \n",
    "# Followers: 163987.4 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
